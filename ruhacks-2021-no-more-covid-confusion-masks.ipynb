{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This Notebook will detail how I created a MultiClass Classification System to identify when you are wearing a mask properly or not(And how to fix it if you are not!).","metadata":{}},{"cell_type":"markdown","source":"# Import Dependencies ","metadata":{}},{"cell_type":"code","source":"%%capture\nimport os\nimport torch\nimport torch.nn as nn \nimport torch.nn.functional as F \nimport torch.optim as optim\nimport torchvision\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport numpy\nimport numpy as np\nimport pandas as pd\nimport json\nimport cv2\n\nimport os\nimport math\nimport copy\nimport random\n\n!pip install performer-pytorch\nimport performer_pytorch\n\n!pip install efficientnet_pytorch\nfrom efficientnet_pytorch import EfficientNet\n!pip install timm\nimport timm\n\n!apt-get update\n!apt-get install libturbojpeg\n!pip install -U git+git://github.com/lilohuang/PyTurboJPEG.git\n\n\n!pip install PyTurboJPEG\nfrom turbojpeg import TurboJPEG\n\nimport pytorch_lightning as pl\nfrom collections import Counter \n\nimport copy\nimport math\n\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfrom fastai.vision.all import *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset ","metadata":{}},{"cell_type":"code","source":"# Reproducibility:\ndef seed_all():\n    seed = 42\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    # Slight Stochasticity Tradeoff for Quicker Comp.\n    torch.backends.cudnn.benchmark = False # True for faster\n    pl.seed_everything(seed)\n    set_seed(42, True)\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)\nseed_all()\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CLASSES = ['Mouth', 'Nose', 'Chin']\nerrors = ['Chin', 'Mouth_Chin', 'Nose_Mouth'] # Uncovered Mouth and Nose, Uncovered Nose, and Uncovered Chin.","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract 5000 examples of pos, 5000 of each error, and 5000 with no mask(25000 images total)\nstarting_dir = 35000\ndef find_images_neg(base_path, categories, num_sample):\n    cur_dir = starting_dir\n    images_selected = {cat: [] for cat in categories}\n    num_selected = {cat: 0 for cat in categories}\n    images = os.listdir(f'{base_path}{cur_dir}')\n    while images != []:\n        for image in images:\n            if 'Mouth_Chin' in image:\n                cat = 'Mouth_Chin'    \n            elif 'Chin' in image:\n                cat = 'Chin'\n            elif 'Nose_Mouth' in image:\n                cat = 'Nose_Mouth'\n            if num_selected[cat] >= num_sample:\n                continue\n            images_selected[cat].append(f\"{base_path}{cur_dir}/{image}\")\n            num_selected[cat] += 1\n        cur_dir += 1000\n        try:\n            images = os.listdir(f'{base_path}{cur_dir}')\n        except:\n            break\n    return images_selected\ndef find_images_pos(base_path, num_samples):\n    # Extracts the first 5000 images \n    cur_dir = starting_dir\n    images_selected = []\n    images = os.listdir(f\"{base_path}{cur_dir}\")\n    while images != []:\n        images_selected += [f\"{base_path}{cur_dir}/{image}\" for image in images]  \n        cur_dir += 1000\n        try:\n            images = os.listdir(f\"{base_path}{cur_dir}\")\n        except:\n            break\n    random.shuffle(images_selected)\n    return images_selected[:num_samples]\ndef create_pandas():\n    # Creates the Data Splits(Loaded in later to avoid reprocessing) \n    no_mask_path = '../input/no-mask/no_mask/'\n    correct_masks_path = '../input/correctmasks1/'\n    incorrect_masks_path = '../input/incorrectmasks1/'\n    \n    \n    num_samples = 20000\n    no_mask_images = [f\"{no_mask_path}{image}\" for image in os.listdir(no_mask_path)] # 5000 examples\n    incorrect_images = find_images_neg(incorrect_masks_path, errors, num_samples)\n    correct_images = find_images_pos(correct_masks_path, num_samples)\n    # Extract Error types\n    chin_images = incorrect_images['Chin']\n    mouth_chin_images = incorrect_images['Mouth_Chin']\n    nose_mouth_images = incorrect_images['Nose_Mouth']\n    # Create pandas dataframe\n    rows = []\n    for image in correct_images:\n        rows += [(image, 1, 1, 1)] # Ordering is Mouth, Chin, Nose\n    for image in chin_images:\n        rows += [(image, 0, 1, 0)]\n    for image in nose_mouth_images:\n        rows += [(image, 1, 0, 1)]\n    for image in mouth_chin_images:\n        rows += [(image, 1, 1, 0)]\n    for image in no_mask_images:\n        rows += [(image, 0, 0, 0)]\n    rows = np.array(rows)\n    df = pd.DataFrame(data = rows, columns = ['image', 'mouth', 'chin', 'nose'])\n    return df\ndf = create_pandas()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    df = df\n    base_path = '../input/covidmask/images/'\n    # Split into Train and Test\n    num_train_samples = -1\n    num_val_samples = 8192\n    train, test = train_test_split(df, train_size = 1 - num_val_samples / len(df), test_size = num_val_samples / len(df), shuffle = True, random_state = 42)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Custom Dataset","metadata":{}},{"cell_type":"code","source":"# Train and Test Transforms\nIMAGE_SIZE = 256\ntrain_transforms = A.Compose([\n    A.RandomResizedCrop(IMAGE_SIZE, IMAGE_SIZE, scale=(0.9, 0.9), p=1),\n    A.HorizontalFlip(p = 0.5),\n    A.OneOf([\n        #A.MotionBlur(blur_limit=(3, 5)),\n        A.Blur(),\n        A.MedianBlur(blur_limit=5),\n        #A.GaussianBlur(blur_limit=(3, 5)),\n        #A.GaussNoise(var_limit=(5.0, 30.0)),\n    ], p=0.7),\n    A.GridDistortion(num_steps=5, distort_limit=0.3, p = 0.25),\n    #A.CLAHE(clip_limit=4.0, p=0.7),\n    #A.IAAPiecewiseAffine(p=0.2),\n    #A.IAASharpen(p=0.2),\n    A.OneOf([\n        A.RandomGamma(gamma_limit=(70, 130), p=1),\n        A.ColorJitter(p=1),\n    ]),\n    #A.OneOf([\n    #    A.ImageCompression(),\n    #    A.Downscale(scale_min=0.7, scale_max=0.95),\n    #], p=0.2),\n    A.Cutout(),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85)    \n])\ntest_transforms = A.Compose([\n    A.Normalize(),\n    ToTensorV2() # I data Preprocessed already, the images are already 256x256.\n])\nclass ValDataset(torch.utils.data.Dataset):\n    def __init__(self, df):\n        self.df = df\n        self.TurboJPEG = TurboJPEG()\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_file = row.image\n        mouth = int(row.mouth)\n        chin = int(row.chin)\n        nose = int(row.nose)\n        \n        stripped = ''\n        for idx in range(len(image_file) - 1, -1, -1):\n            if image_file[idx] == '/':\n                stripped =  image_file[idx + 1:]\n                break\n        image_file = Config.base_path + stripped\n        GT = torch.tensor([mouth, chin, nose])\n        \n        with open(image_file, 'rb') as file:\n            image = self.TurboJPEG.decode(file.read())\n        \n        image = test_transforms(image = image)['image']\n        return image, GT\nclass TrainDataset(torch.utils.data.Dataset):\n    def __init__(self, df, num_samples):\n        self.df = df\n        self.num_samples = num_samples\n        self.actual_length = len(self.df)\n        if self.num_samples == -1:\n            self.num_samples = self.actual_length\n        self.TurboJPEG = TurboJPEG()\n    def __len__(self):\n        return self.num_samples\n    def __getitem__(self, idx):\n        if self.num_samples != self.actual_length:\n            idx = random.randint(0, self.actual_length - 1) # Random Sample.\n        row = self.df.iloc[idx]\n        \n        image_file = row.image\n        stripped = ''\n        for idx in range(len(image_file) - 1, -1, -1):\n            if image_file[idx] == '/':\n                stripped = image_file[idx + 1: ]\n                break\n        image_file = Config.base_path + stripped\n        \n        mouth = int(row.mouth)\n        chin = int(row.chin)\n        nose = int(row.nose)\n        arr = [mouth, chin, nose]\n        # Sanity Check GT's\n        \n        if arr == [1, 1, 0]:\n            assert 'Mouth_Chin' in image_file, image_file\n        if arr == [0, 1, 0]:\n            assert \"Mask_Chin\" in image_file, image_file\n        if arr == [1, 0, 1]:\n            assert 'Nose_Mouth' in image_file, image_file\n        GT = torch.tensor(arr)\n        with open(image_file, 'rb') as file:\n            image = self.TurboJPEG.decode(file.read())\n        \n        image = train_transforms(image = image)['image']\n        image = test_transforms(image = image)['image']\n        return image, GT\nclass DataModule:\n    @classmethod\n    def get_both(cls):\n        train_dataset = TrainDataset(Config.train, Config.num_train_samples)\n        test_dataset = ValDataset(Config.test)\n        return train_dataset, test_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Construction(EffNet B0 + ViT + ResNext elements)","metadata":{}},{"cell_type":"code","source":"def initialize_weights(layer):\n    for module in layer.modules():\n        if isinstance(module, (nn.Conv2d, nn.Conv1d)):\n            # initialize using Kaiming Normal\n            nn.init.kaiming_normal_(module.weight, nonlinearity = 'relu')\n        elif isinstance(module, (nn.BatchNorm2d)):\n            module.weight.data.fill_(1)\n            module.bias.data.zero_()\nclass Mish(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        return torch.tanh(F.softplus(x))\ndef replace_all_act(model):\n    for name, module in model.named_modules():\n        if isinstance(module, (nn.ReLU, nn.SiLU)):\n            # Replace with Mish\n            setattr(module, name, Mish())\n        else:\n            replace_all_act(module)\nclass Act(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.act_type = ModelConfig.act\n        if self.act_type == 'relu':\n            self.act = nn.ReLU(inplace = True)\n        elif self.act_type == 'silu':\n            self.act = nn.SiLU(inplace = True)\n        else:\n            self.act = Mish()\n    def forward(self, x):\n        return self.act(x)\nclass ConvBlock(pl.LightningModule):\n    def __init__(self, in_features,  out_features, kernel_size, padding, groups, stride):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride)\n        self.bn = nn.BatchNorm2d(out_features)\n        self.act = Act()\n        initialize_weights(self)\n    def forward(self, x):\n        return self.bn(self.act(self.conv(x)))\nclass SqueezeExcite(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        \n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.Squeeze = nn.Linear(self.in_features, self.inner_features)\n        self.Act = Act()\n        self.Excite = nn.Linear(self.inner_features, self.in_features)\n    def forward(self, x):\n        pool = torch.squeeze(self.avg_pool(x))\n        squeeze = self.Act(self.Squeeze(pool))\n        excite = torch.sigmoid(self.Excite(squeeze)).unsqueeze(-1).unsqueeze(-1)\n        return excite * x\nclass ECASqueezeExcite(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.kernel_size = 5\n        self.padding = 2\n        \n        self.conv1 = nn.Conv1d(1, 1, kernel_size = self.kernel_size, padding = self.padding, bias = False)\n        self.avgPool = nn.AdaptiveAvgPool2d((1, 1))\n        initialize_weights(self)\n    def forward(self, x):\n        avg_pool = torch.squeeze(self.avgPool(x), dim = -1).transpose(-1, -2) # (B, 1, C)\n        excite = torch.sigmoid(self.conv1(avg_pool)).transpose(-1, -2).unsqueeze(-1) # (B, C, 1, 1)\n        return excite * x\nclass SCSE(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features \n        \n        self.avgPool = nn.AdaptiveAvgPool2d((1, 1))\n        self.Squeeze = nn.Linear(self.in_features, self.inner_features)\n        self.Act = Act()\n        self.Excite = nn.Linear(self.inner_features, self.in_features)\n        \n        self.Spatial = nn.Conv2d(self.in_features, 1, kernel_size = 1)\n        initialize_weights(self)\n    def forward(self, x):\n        pooled = torch.squeeze(self.avgPool(x))\n        squeeze = self.Act(self.Squeeze(pooled))\n        excite = torch.sigmoid(self.Excite(squeeze)).unsqueeze(-1).unsqueeze(-1) * x\n        \n        excite_conv = torch.sigmoid(self.Spatial(x)) * x\n        excited = (excite + excite_conv) / 2\n        return excited\nclass Attention(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.attention_type = ModelConfig.attention_type\n        assert self.attention_type in ['eca', 'none', 'se', 'scse']\n        if self.attention_type == 'eca':\n            self.layer = ECASqueezeExcite()\n        elif self.attention_type == 'se':\n            self.layer = SqueezeExcite(in_features, inner_features)\n        elif self.attention_type == 'scse':\n            self.layer = SCSE(in_features, inner_features)\n        else:\n            self.layer = nn.Identity()\n        self.gate_attention = ModelConfig.gate_attention\n        if self.gate_attention:\n            self.gamma = nn.Parameter(torch.zeros((1), device = self.device) - 10) # Init to residual connection at first.\n    def forward(self, x):\n        val = self.layer(x)\n        if self.gate_attention:\n            gamma = torch.sigmoid(self.gamma)\n            return gamma * val + (1 - gamma)\n        return val\nclass MultiHeadedAttention(pl.LightningModule):\n    def __init__(self, in_features, inner_features, num_heads):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.num_heads = num_heads\n    \n        self.K = nn.Linear(self.in_features, self.inner_features)\n        self.V = nn.Linear(self.in_features, self.inner_features)\n        self.Q = nn.Linear(self.in_features, self.inner_features)\n        \n        self.Linear = nn.Linear(self.in_features, self.inner_features)\n    def forward(self, x):\n        B, L, C = x.shape\n        Keys = self.K(x)\n        Values = self.V(x)\n        Queries = self.Q(x) # (B, L, HI)\n        \n        Keys = Keys.reshape(B, L, self.num_heads, self.inner_features)\n        Values = Values.reshape(B, L, self.num_heads, self.inner_features)\n        Queries = Queries.reshape(B, L, self.num_heads, self.inner_features)\n        \n        Keys = Keys.transpose(1, 2)\n        Values = Values.transpose(1, 2)\n        Queries = Queries.transpose(1, 2) # (B, H, L, I)\n        \n        Keys = Keys.reshape(B * self.num_heads, L, self.inner_features)\n        Values = Values.reshape(B * self.num_heads, L, self.inner_features)\n        Queries = Queries.reshape(B * self.num_heads, L, self.inner_features) # (BH, L, I)\n        \n        att_mat = F.softmax(torch.bmm(Keys, Values.transpose(1, 2)) / math.sqrt(self.inner_features))\n        att_scores = torch.bmm(att_mat, Queries) #(BH, L, C)\n        \n        att_scores = att_scores.reshape(B, self.num_heads, L, self.inner_features)\n        att_scores = att_scores.transpose(1, 2)\n        att_scores = att_scores.reshape(B, L, self.num_heads * self.inner_features)\n        return self.Linear(att_scores)\nclass PerformerAttention(pl.LightningModule):\n    def __init__(self, in_features, inner_features, num_heads):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.num_heads = num_heads\n        \n        self.K = nn.Linear(self.in_features, self.inner_features * self.num_heads)\n        self.V = nn.Linear(self.in_features, self.inner_features * self.num_heads)\n        self.Q = nn.Linear(self.in_features, self.inner_features * self.num_heads)\n        self.Linear = nn.Linear(self.inner_features * self.num_heads, self.in_features)\n        \n        self.att = performer_pytorch.FastAttention(\n            dim_heads = self.inner_features,\n            nb_features = self.inner_features\n        )\n    def forward(self, x):\n        B, L, C = x.shape\n        Keys = self.K(x)\n        Values = self.V(x)\n        Queries = self.Q(x) # (B, L, HI)\n        \n        Keys = Keys.reshape(B, L, self.num_heads, self.inner_features)\n        Values = Values.reshape(B, L, self.num_heads, self.inner_features)\n        Queries = Queries.reshape(B, L, self.num_heads, self.inner_features)\n        \n        Keys = Keys.transpose(1, 2)\n        Values = Values.transpose(1, 2)\n        Queries = Queries.transpose(1, 2) # (B, H, L, self.inner_features)\n        \n        attended = self.att(Queries, Keys, Values)\n        attended = attended.reshape(B, self.num_heads, L, self.inner_features)\n        attended = attended.transpose(1, 2)\n        attended = attended.reshape(B, L, -1)\n        return self.Linear(attended)\nclass SelfAttention(pl.LightningModule):\n    def __init__(self, in_features, inner_features, num_heads):\n        super().__init__()\n        self.attention_type = ModelConfig.self_attention_type\n        if self.attention_type == 'performer':\n            self.layer = PerformerAttention(in_features, inner_features, num_heads)\n        else:\n            self.layer = MultiHeadedAttention(in_features, inner_features, num_heads)\n    def forward(self, x):\n        return self.layer(x)\nclass TransformerEncoder(pl.LightningModule):\n    def __init__(self, in_features, inner_features, num_heads):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.num_heads = num_heads\n    \n        self.length = ModelConfig.max_length\n        self.LayerNorm1 = nn.LayerNorm((self.length, self.in_features))\n        self.SA = SelfAttention(self.in_features, self.inner_features, self.num_heads)\n        self.LayerNorm2 = nn.LayerNorm((self.length, self.in_features))\n        self.Linear = nn.Linear(self.in_features, self.in_features)\n    def forward(self, x):\n        norm1 = self.LayerNorm1(x)\n        SA = self.SA(norm1) + x \n        norm2 = self.LayerNorm2(SA)\n        linear = self.Linear(norm2) + SA\n        return linear\n        \nclass AstrousConvBlock(pl.LightningModule):\n    def __init__(self, in_features, out_features, kernel_size, padding, groups, stride, dilation):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride, dilation = dilation)\n        self.bn = nn.BatchNorm2d(out_features)\n        self.act = Act()\n        initialize_weights(self)\n    def forward(self, x):\n        return self.bn(self.act(self.conv(x)))\nclass BAM(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.bam_dilate = ModelConfig.bam_dilate\n    \n        self.avgPool = nn.AdaptiveAvgPool2d((1, 1))\n        self.Squeeze = nn.Linear(self.in_features, self.inner_features)\n        self.act = Act()\n        self.Excite = nn.Linear(self.inner_features, self.in_features)\n        \n        self.Squeeze_Conv = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.DA = AstrousConvBlock(self.inner_features, self.inner_features, 3, self.bam_dilate, self.inner_features, 1, self.bam_dilate)\n        self.Excite_Conv = ConvBlock(self.inner_features, 1, 1, 0, 1, 1)\n        \n        self.gate_attention = ModelConfig.gate_attention\n        if self.gate_attention:\n            self.gamma = nn.Parameter(torch.zeros((1), device = self.device) - 10)\n    def forward(self, x):\n        pooled = torch.squeeze(self.avgPool(x))\n        squeeze = self.act(self.Squeeze(pooled))\n        excite = torch.sigmoid(self.Excite(squeeze)).unsqueeze(-1).unsqueeze(-1) * x\n        \n        squeeze_conv = self.Squeeze_Conv(x)\n        DA = self.DA(squeeze_conv)\n        excite_conv = torch.sigmoid(self.Excite_Conv(DA)) \n        excite_conv = excite_conv * x\n        excited = (excite + excite_conv) / 2\n        if self.gate_attention:\n            gamma = torch.sigmoid(self.gamma)\n            return excited * gamma + (1 - gamma) * x\n        return excited\nclass SplitAttention(pl.LightningModule):\n    # Basic Implementation of Split Attention in ResNeSt\n    def __init__(self, in_features, inner_features, cardinality):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.cardinality = cardinality\n        \n        assert self.inner_features % self.cardinality == 0\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.Squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.Excite = nn.Conv2d(self.inner_features, self.in_features * self.cardinality, kernel_size =1, groups = self.cardinality)\n        \n        self.gate_attention = ModelConfig.gate_attention\n        if self.gate_attention:\n            self.gamma = nn.Parameter(torch.zeros((1), device = self.device) - 10)\n        initialize_weights(self)\n    def forward(self, x):\n        '''\n        x: Tensor(B, C, H, W, Cardinality), where Cardinality is the number of groups to apply split attention to.\n        '''\n        B, C, H, W, Cardinality = x.shape\n        assert Cardinality == self.cardinality\n        # Sum across all groups\n        summed = torch.sum(x, dim = -1) # (B, C, H, W)\n        # Pool\n        pooled = self.global_pool(summed) # (B, C, 1, 1)\n        # Conv\n        squeeze = self.Squeeze(pooled) # (B, I, 1, 1)\n        excite = torch.squeeze(self.Excite(squeeze)) # (B, Cardinality * Channels)\n        \n        excite = F.softmax(excite.reshape(B, C, Cardinality), dim = -1) \n        excite = excite.unsqueeze(2).unsqueeze(2) # (B, C, 1, 1, Cardinality)\n        if self.gate_attention:\n            gamma = torch.sigmoid(self.gamma)\n            excited = excite * x * gamma + (1 - gamma) * x\n        else:\n            excited = excite * x # (B, C, H, W, Cardinality)\n        \n        excited = torch.sum(excited, dim = -1) # (B, C, H, W)\n        return excited\nclass ResNext(pl.LightningModule):\n    '''\n    ResNext Block. I would make ResNest, but that is just ResNext + SplitAttention(implemented above)\n    '''\n    def __init__(self, in_features, inner_features, cardinality):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.cardinality = cardinality\n        self.reduction = ModelConfig.reduction\n        \n        self.Squeeze = ConvBlock(self.in_features, self.inner_features * self.cardinality, 1, 0, 1, 1)\n        self.Process = ConvBlock(self.inner_features * self.cardinality, self.inner_features * self.cardinality, 3, 1, self.cardinality, 1)\n        self.Expand = ConvBlock(self.inner_features * self.cardinality, self.in_features, 1, 0, 1, 1)\n        self.SE = Attention(self.in_features, self.in_features // self.reduction)\n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device) - 10)\n    def forward(self, x):\n        squeeze = self.Squeeze(x)\n        process = self.Process(squeeze)\n        expand = self.Expand(process)\n        SE = self.SE(expand)\n        gamma = torch.sigmoid(self.gamma)\n        return gamma * SE + (1 - gamma) * x\nclass BottleNeck(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.reduction = ModelConfig.reduction\n        \n        self.Squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.Process = ConvBlock(self.inner_features, self.inner_features, 3, 1, 1, 1)\n        self.Expand = ConvBlock(self.inner_features, self.in_features, 1, 0, 1, 1)\n        self.SE = Attention(self.in_features, self.in_features // self.reduction)\n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device) - 10)\n    def forward(self, x):\n        \n        squeeze = self.Squeeze(x)\n        process = self.Process(squeeze)\n        expand = self.Expand(process)\n        excited = self.SE(expand) \n        \n        gamma = torch.sigmoid(excited)\n        return excited * gamma + (1 - gamma) * x\n    \nclass DownsamplerBottleNeck(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, stride):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.stride = stride\n        self.reduction = ModelConfig.reduction\n        \n        self.avgPool = nn.AvgPool2d(kernel_size = 3, padding = 1, stride = self.stride)\n        self.ConvAvg = ConvBlock(self.in_features, self.out_features, 1, 0, 1, 1)\n    \n        self.Squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.Process = ConvBlock(self.inner_features, self.inner_features, 3, 1, 1, self.stride)\n        self.Expand = ConvBlock(self.inner_features, self.out_features, 1, 0, 1, 1)\n        self.SE = Attention(self.out_features, self.out_features // self.reduction)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device) - 10)\n    def forward(self, x):\n        pooled = self.avgPool(x)\n        conv_pool = self.ConvAvg(pooled)\n        \n        squeeze = self.Squeeze(x)\n        process = self.Process(squeeze)\n        expand = self.Expand(process)\n        SE = self.SE(expand)\n        \n        gamma = torch.sigmoid(self.gamma)\n        return SE * gamma + (1 - gamma) * conv_pool\nclass InverseBottleNeck(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.reduction = ModelConfig.reduction\n        \n        self.Expand = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.DW = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features, 1)\n        self.SE = Attention(self.inner_features, self.inner_features // self.reduction)\n        self.Squeeze = ConvBlock(self.inner_features, self.in_features, 1, 0, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device) - 10)\n    def forward(self, x):\n        expand = self.Expand(x)\n        DW = self.DW(expand)\n        SE = self.SE(DW)\n        squeeze = self.Squeeze(SE)\n        \n        gamma = torch.sigmoid(self.gamma)\n        return gamma * squeeze + (1 - gamma) * x\nclass DownsamplerInverseBottleNeck(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, stride):\n        super().__init__()\n        self.in_features = in_features \n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.stride = stride\n        self.reduction = ModelConfig.reduction\n        \n        self.AvgPool = nn.AvgPool2d(kernel_size = 3, padding = 1, stride = self.stride)\n        self.ConvPool = ConvBlock(self.in_features, self.out_features, 1, 0, 1, 1)\n    \n        self.Expand = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.DW = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features, self.stride)\n        self.SE = Attention(self.inner_features, self.inner_features // self.reduction)\n        self.Squeeze = ConvBlock(self.inner_features, self.out_features, 1, 0, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device) - 10)\n    def forward(self, x):\n        pooled = self.AvgPool(x)\n        convPool = self.ConvPool(pooled)\n        \n        expand = self.Expand(x)\n        DW = self.DW(expand)\n        SE = self.SE(DW)\n        squeeze = self.Squeeze(SE)\n        \n        gamma = torch.sigmoid(self.gamma) \n        return gamma * squeeze + (1 - gamma) * convPool\nclass GhostConvBlock(pl.LightningModule):\n    # Ghost ConvBlock - Good Performance for Little Params\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        assert self.out_features % 2 == 0\n        self.inner_features = self.out_features // 2\n        \n        self.Squeeze = nn.Conv2d(self.in_features, self.inner_features, kernel_size = 1)\n        self.DW = nn.Conv2d(self.inner_features, self.inner_features, kernel_size = 1, groups = self.inner_features)\n        self.BN = nn.BatchNorm2d(self.inner_features * 2)\n        self.act = Act()\n    def forward(self, x):\n        squeeze = self.Squeeze(x)\n        DW = self.DW(squeeze)\n        concat = torch.cat([squeeze, DW], dim = 1)\n        return self.BN(self.act(concat))\nclass GhostBottleNeck(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.reduction = ModelConfig.reduction\n        \n        self.Conv1 = GhostConvBlock(self.in_features, self.inner_features)\n        self.Conv2 = GhostConvBlock(self.inner_features, self.in_features)\n        self.Attention = Attention(self.in_features, self.in_features // self.reduction)\n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device) - 10)\n    def forward(self, x):\n        conv1 = self.Conv1(x)\n        conv2 = self.Conv2(conv1)\n        conv2 = self.Attention(conv2)\n        gamma = torch.sigmoid(self.gamma)\n        return gamma * conv2 + (1 - gamma) * x\nclass DownsamplerGhostBottleNeck(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, stride):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features= out_features\n        self.stride = stride\n        self.reduction = ModelConfig.reduction\n        \n        self.AvgPool = nn.AvgPool2d(kernel_size = 3, padding = 1, stride = self.stride)\n        self.GhostPool = GhostConvBlock(self.in_features, self.out_features)\n        \n        self.Ghost1 = GhostConvBlock(self.in_features, self.inner_features)\n        self.DW = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features, self.stride)\n        self.Ghost2 = GhostConvBlock(self.inner_features, self.out_features)\n        self.Attention = Attention(self.out_features, self.out_features // self.reduction)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device) - 10)\n    def forward(self, x):\n        pooled = self.AvgPool(x)\n        ghost_pool = self.GhostPool(pooled)\n        \n        ghost_1 = self.Ghost1(x)\n        dw = self.DW(ghost_1)\n        ghost_2 = self.Ghost2(dw)\n        attention = self.Attention(ghost_2)\n        \n        gamma = torch.sigmoid(self.gamma)\n        return gamma * attention + (1 - gamma) * ghost_pool\n        \nclass ChooseBottleNeck(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.bottleneck_type = ModelConfig.bottleneck_type\n        assert self.bottleneck_type in ['ghost', 'inverse', 'bottleneck']\n        if self.bottleneck_type == 'ghost':\n            self.layer = GhostBottleNeck(in_features, inner_features)\n        elif self.bottleneck_type == 'inverse':\n            self.layer = InverseBottleNeck(in_features, inner_features)\n        else:\n            self.layer = BottleNeck(in_features, inner_features)\n    def forward(self, x):\n        return self.layer(x)\nclass ChooseDownsampler(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, stride):\n        super().__init__()\n        self.bottleneck_type = ModelConfig.bottleneck_type\n        assert self.bottleneck_type in ['ghost', 'inverse', 'bottleneck']\n        if self.bottleneck_type == 'ghost':\n            self.layer = DownsamplerGhostBottleNeck(in_features, inner_features, out_features, stride)\n        elif self.bottleneck_type == 'inverse':\n            self.layer = DownsamplerInverseBottleNeck(in_features, inner_features, out_features, stride)\n        else:\n            self.layer = DownsamplerBottleNeck(in_features, inner_features, out_features, stride)\n    def forward(self, x):\n        return self.layer(x)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model construction","metadata":{}},{"cell_type":"code","source":"class FeatureExtractor(pl.LightningModule):\n    def freeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = False\n    def __init__(self):\n        super().__init__()\n        self.model_name = 'efficientnet-b0'\n        self.model = EfficientNet.from_pretrained(self.model_name)\n        # Extract Layers\n        self.conv1 = self.model._conv_stem\n        self.bn1 = self.model._bn0\n        self.act1 = self.model._swish\n        self.enc_dims = [3, 32, 24, 40, 80, 112, 192, 320]\n        \n        self.block0 = self.model._blocks[0]\n        self.block1 = nn.Sequential(*self.model._blocks[1:3])\n        self.block2 = nn.Sequential(*self.model._blocks[3:5])\n        self.block3 = nn.Sequential(*self.model._blocks[5:8])\n        self.block4 = nn.Sequential(*self.model._blocks[8:11])\n        self.block5 = nn.Sequential(*self.model._blocks[11:15])\n        self.block6 = self.model._blocks[15]\n        \n        self.freeze([self.conv1, self.bn1, self.block0, self.block1, self.block2, self.block3, self.block4])\n        # Custom Layers\n        self.reduction = ModelConfig.reduction\n        \n        self.Attention6 = BAM(self.enc_dims[-1], self.enc_dims[-1] // self.reduction)\n        self.Dropout6 = nn.Dropout2d(0.1)\n        \n        self.out_dim = ModelConfig.out_dim\n        self.num_blocks = ModelConfig.num_blocks\n        self.block7 = nn.Sequential(*[\n            ChooseDownsampler(self.enc_dims[-1], self.enc_dims[-1] // self.reduction, self.out_dim, 2)\n        ] + [\n            ChooseBottleNeck(self.out_dim, self.out_dim // self.reduction) for i in range(self.num_blocks)\n        ])\n        \n        self.Attention7 = BAM(self.out_dim, self.out_dim // self.reduction)\n        self.Dropout7 = nn.Dropout2d(0.1)\n    \n    def forward(self, x):\n        features0 = self.bn1(self.act1(self.conv1(x))) # (B, 32, 128, 128)\n        block0 = self.block0(features0) # (b, 16, 128, 128)\n        block1 = self.block1(block0) # (B, 24, 64, 64)\n        block2 = self.block2(block1) # (B, 40, 32, 32)\n        block3 = self.block3(block2) # (B, 80, 16, 16)\n        block4 = self.block4(block3) # (b, 112, 16, 16)\n        block5 = self.block5(block4) # (B, 192, 8, 8)\n        block6 = self.block6(block5) # (B, 320, 8, 8)\n        # Custom Layer\n        block6 = self.Attention6(self.Dropout6(block6))\n        \n        block7 = self.block7(block6)\n        block7 = self.Attention7(self.Dropout7(block7))\n        \n        return block7 # (B, 512, 4, 4)\n        \n        ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BaseLineHead(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.num_classes = ModelConfig.num_classes\n        self.out_dim = ModelConfig.out_dim\n        self.global_avg = nn.AdaptiveAvgPool2d((1, 1))\n        self.Linear = nn.Linear(self.out_dim, self.num_classes)\n    def forward(self, x):\n        avg = torch.squeeze(self.global_avg(x))\n        return self.Linear(avg)\nclass ViTAlphaHead(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.num_classes = ModelConfig.num_classes\n        self.out_dim = ModelConfig.out_dim\n        self.num_encoder = ModelConfig.num_encoder\n        self.max_length = ModelConfig.max_length\n        self.num_heads = ModelConfig.num_heads\n        self.positional_encodings = self.pos_enc().to(self.device)\n        \n        self.encoders = nn.Sequential(*[\n            TransformerEncoder(self.out_dim, self.out_dim // self.num_heads, self.num_heads) for i in range(self.num_encoder)\n        ])\n        \n        self.Linear = nn.Linear(self.out_dim, self.num_classes) \n    def pos_enc(self):\n        L, C = self.max_length, self.out_dim\n        pos_enc = torch.zeros((L, C), device = self.device)\n        for pos in range(L):\n            for i in range(0, C, 2):\n                pos_enc[pos, i] = math.sin(pos / 10000 ** (2 * i / self.out_dim))\n                pos_enc[pos, i + 1] = math.cos(pos / 10000 ** (2 * (i + 1) / self.out_dim))\n        return pos_enc \n    def forward(self, x):\n        B, C, H, W = x.shape\n        assert H * W == self.max_length\n        # Flatten\n        flat_input = x.reshape(B, C, H * W).transpose(1, 2) # (B, L, C)\n        # Add Positional Encodings\n        positional = torch.repeat_interleave(self.positional_encodings.unsqueeze(0), B, dim = 0).to(self.device) + flat_input\n        # Encode using transformers\n        encoded = self.encoders(positional)\n        # Avg Pool\n        pooled = torch.mean(encoded, dim = 1)\n        return self.Linear(pooled)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ViTAlpha(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.feature_extractor = FeatureExtractor()\n        self.head_type = ModelConfig.head_type\n        self.head = BaseLineHead() if self.head_type == 'baseline' else ViTAlphaHead()\n    def forward(self, x):\n        features = self.feature_extractor(x)\n        return self.head(features)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Config","metadata":{}},{"cell_type":"code","source":"class ModelConfig:\n    head_type = 'transformer'\n    num_encoder = 3\n    num_heads = 4\n    \n    num_classes = 3\n    act = 'relu'\n    attention_type = 'scse'\n    self_attention_type = 'performer' # More Efficient, Memory Light.\n    gate_attention = True # Stabilizes Attention, allowing the model to converge slightly faster\n    \n    bottleneck_type = 'ghost'\n    max_length = 16 # Predetermined, based on the CNN encoder.\n    bam_dilate = 3\n    reduction = 2\n    \n    out_dim = 512\n    num_blocks = 2","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training Config","metadata":{}},{"cell_type":"code","source":"class TrainConfig:\n    batch_size = 384\n    weight_decay = 0\n    num_workers = 4\n    num_epochs = 100 # Dummy Number, never reached.\n    \n    lr = 1e-4\n    num_steps = 5\n    eta_min = 1e-7\n    patience = 3\n    factor= 0.2\n    clip_grads = 20","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Store():\n    def __init__(self, y_pred, y_true):\n        self.pred = y_pred\n        self.y = y_true\nclass Loss(Metric):\n    def __init__(self):\n        self.loss = 0\n        self.count = 0\n        self.criterion = nn.BCEWithLogitsLoss()\n    def reset(self):\n        self.loss = 0\n        self.count = 0\n    def accumulate(self, learn):\n        pred, y = torch.sigmoid(learn.pred), learn.y\n        y = y.float()\n        loss = self.criterion(pred, y)\n        self.loss += loss.item()\n        self.count += 1\n        return loss\n    @property \n    def value(self):\n        if self.count != 0:\n            return round(self.loss / self.count, 3)\n        return 0\nclass F1Score(Metric):\n    def __init__(self):\n        self.f1_score = 0\n        self.count = 0\n    def reset(self):\n        self.count = 0\n        self.f1_score = 0\n    def round_(self, y_pred):\n        ones = y_pred >= 0.5\n        y_pred[:, :] = 0\n        y_pred[ones] = 1\n        return y_pred\n    def compute_f1(self, y_pred, y_true):\n        eps = 1e-7\n        tp = torch.sum(y_pred * y_true) * 2 + eps\n        fp = torch.sum(y_pred + y_true) + eps\n        return tp / fp\n    \n    def accumulate(self, learn):\n        pred, y = torch.sigmoid(learn.pred), learn.y\n        pred = self.round_(pred)\n        \n        f1_score = self.compute_f1(y, pred).item()\n        self.f1_score += f1_score\n        self.count +=1\n    @property\n    def value(self):\n        if self.count != 0:\n            return round(self.f1_score / self.count, 3)\n        return 0\nclass Accuracy(Metric):\n    def __init__(self):\n        self.accuracy = 0\n        self.count = 0\n    def reset(self):\n        self.count = 0\n        self.accuracy = 0\n    def round_(self, y_pred):\n        ones = y_pred >= 0.5\n        y_pred[:, :] = 0\n        y_pred[ones] = 1\n        return y_pred\n    def accumulate(self, learn):\n        pred, y = torch.sigmoid(learn.pred), learn.y\n        pred = self.round_(pred)\n        B, C = pred.shape\n        accuracy = torch.sum(pred == y) / B / C\n        self.accuracy += accuracy.item()\n        self.count += 1\n    @property\n    def value(self):\n        if self.count != 0:\n            return round(self.accuracy / self.count, 3)\n        return 0\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestingModelAlpha(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = self.configure_model()\n    def configure_model(self):\n        model = ViTAlpha()\n        return model\n    def forward(self, x):\n        self.eval()\n        with torch.no_grad():\n            pred = torch.sigmoid(self.model(x))\n            return pred > 0.5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainingModelAlpha(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = self.configure_model()\n        self.best = {'val_loss': float('inf'), 'val_f1': 0, 'val_accuracy': 0}\n        self.TrainLoss = Loss()\n        self.ValLoss = Loss()\n        self.ValF1 = F1Score()\n        self.ValAccuracy = Accuracy()\n        self.EPOCHS = -1\n        self.reset()\n    def reset(self):\n        self.TrainLoss.reset()\n        self.ValLoss.reset()\n        self.ValF1.reset()\n        self.ValAccuracy.reset()\n        self.EPOCHS += 1\n    def forward(self, x):\n        return self.model(x)\n    def configure_model(self):\n        model = ViTAlpha()\n        return model\n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.model.parameters(), lr = TrainConfig.lr, weight_decay = TrainConfig.weight_decay)\n        self.lr_decay_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, TrainConfig.num_steps, eta_min = TrainConfig.eta_min)\n        self.lr_decay_plateau = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'max', patience = TrainConfig.patience, factor= TrainConfig.factor)\n        return optimizer\n                                                                \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        pred = self.model(x)\n        store = Store(pred, y)\n        loss = self.TrainLoss.accumulate(store)\n        return loss\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        pred = self.model(x)\n        store = Store(pred, y)\n        self.ValLoss.accumulate(store)\n        self.ValF1.accumulate(store)\n        self.ValAccuracy.accumulate(store)\n    def round_states(self):\n        trainLoss = self.TrainLoss.value\n        valLoss = self.ValLoss.value\n        valF1 = self.ValF1.value\n        valAccuracy = self.ValAccuracy.value\n        \n        self.log('f1', valF1)\n        self.lr_decay_plateau.step(valF1)\n        self.lr_decay_cosine.step()\n        print('=====================================')\n        if valLoss <= self.best['val_loss']:\n            self.best['val_loss'] = valLoss\n            torch.save(self.state_dict(), f\"./loss.pth\")\n        if valF1 >= self.best['val_f1']:\n            self.best['val_f1'] = valF1\n            torch.save(self.state_dict(), f\"./f1.pth\")\n        if valAccuracy >= self.best['val_accuracy']:\n            self.best['val_accuracy'] = valAccuracy\n            torch.save(self.state_dict(), f'./acc.pth')\n        print(f\"E: {self.EPOCHS}, BL: {self.best['val_loss']} BF: {self.best['val_f1']}, BA: {self.best['val_accuracy']} TL: {trainLoss} VL: {valLoss}, VA: {valAccuracy} VF: {valF1}\")\n        \n    def validation_epoch_end(self, logs):\n        self.round_states()\n        self.reset()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train():\n    train, test = DataModule.get_both()\n    model = TrainingModelAlpha()\n    dls = DataLoaders.from_dsets(train, test, batch_size = TrainConfig.batch_size, shuffle = True, num_workers = TrainConfig.num_workers, worker_init_fn = seed_worker)\n    if torch.cuda.is_available(): model.cuda, dls.cuda()\n    cbs = [pl.callbacks.EarlyStopping(monitor = 'f1', \n        mode = 'max', \n        patience = 5,\n        verbose = True                      \n    )]\n    trainer = pl.Trainer(callbacks = cbs, num_sanity_val_steps = 0, gpus = 1, deterministic = True, benchmark = False, max_epochs = TrainConfig.num_epochs, check_val_every_n_epoch = 1, checkpoint_callback = False, gradient_clip_val = TrainConfig.clip_grads, logger = False, precision = 16)\n    trainer.fit(model, dls[0], dls[1])\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Script on validation set","metadata":{}},{"cell_type":"code","source":"def decode(pred):\n    present = []\n    if pred[0] == 1: #Mouth, Chin, Nose\n        present += ['Mouth']\n    if pred[1] == 1:\n        present += ['Chin']\n    if pred[2] == 1:\n        present += ['Nose']\n    return present\ndef predict(model, img_path):\n    image = cv2.imread(img_path)\n    assert image is not None\n    image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    plt.imshow(image)\n    plt.show()\n    image = test_transforms(image = image)['image'].unsqueeze(0)\n    pred = torch.squeeze(model(image))\n    print(decode(pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TestingModelAlpha()\nmodel.load_state_dict(torch.load('../input/masks-model/f1.pth', map_location = device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, val = DataModule.get_both()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for images, labels in val:\n#    plt.imshow(images.transpose(0, 1).transpose(1, 2))\n#    plt.show()\n#    print('-------------GT-----------------')\n#    print(decode(labels))\n#    pred = model(images.unsqueeze(0))\n#    print('-----------Pred---------------')\n#    print(decode(labels))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fail2 = '../input/maskstest/Fail2.jpg'\nfail3 = '../input/maskstest/Fail3.jpg'\ngood1 = '../input/maskstest/Good.jpg'\ngood2 = '../input/maskstest/Good2.jpg'\nall_inputs = [fail2, fail3, good1, good2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for input_val in all_inputs:\n    predict(model, input_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}