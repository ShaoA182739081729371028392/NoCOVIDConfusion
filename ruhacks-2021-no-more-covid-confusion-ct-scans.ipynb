{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook covers the second portion of my submission for RUHacks, COVID-19 Classification from CT Scans.\n\nI use FastAI for quickly building models, but ultimately I use PyTorch Lightning to organize code after it is prepared and ready for a full train(Learners abstract away too many details)","metadata":{}},{"cell_type":"markdown","source":"# Import Dependencies","metadata":{}},{"cell_type":"code","source":"%%capture\nimport os\nimport torch\nimport torch.nn as nn \nimport torch.nn.functional as F \nimport torch.optim as optim\nimport torchvision\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n!pip install performer-pytorch\nimport performer_pytorch\n\nimport numpy\nimport numpy as np\nimport pandas as pd\nimport json\nimport cv2\n\nimport os\nimport math\nimport copy\nimport random\n\n!pip install timm\nimport timm\n\n!pip install efficientnet_pytorch\nfrom efficientnet_pytorch import EfficientNet\n!pip install lycon\nimport lycon\n\nimport pytorch_lightning as pl\nfrom collections import Counter \n\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfrom fastai.vision.all import *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Data","metadata":{}},{"cell_type":"code","source":"# Reproducibility:\ndef seed_all():\n    seed = 42\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    # Slight Stochasticity Tradeoff for Quicker Comp.\n    torch.backends.cudnn.benchmark = False # True for faster\n    pl.seed_everything()\n    set_seed(42, True)\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)\nseed_all()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_df(false_path, true_path):\n    # Creates a Pandas Dataframe for Classification\n    all_false= [(f\"{false_path}{path}\", 1) for path in os.listdir(false_path)] # 1 Means COVID\n    all_true = [(f\"{true_path}{path}\", 0) for path in os.listdir(true_path)] # 0 means non-covid\n    all_items = all_false + all_true \n    df = pd.DataFrame(all_items, columns = ['image', 'covid'])\n    df = df.set_index('image')\n    df.to_csv('./split.csv')\nclass Config:\n    false_path = '../input/covid-ct/false/false/'\n    true_path = '../input/covid-ct/true/true/'\n    data_split = '../input/covidsplit/split.csv' # Generated by the above function\n    \n    df = pd.read_csv(data_split) # 2500 Examples in total\n    train, test = train_test_split(df, train_size = 0.9, test_size = 0.1, shuffle = True, random_state = 42)\n    \n    num_train_samples = 4096\nclass TrainingConfig:\n    # Handles Hyper Params for Training\n    use_SWA = False # Stochastic weight Averaging\n    lr = 1e-3\n    num_steps = 5\n    factor = 0.1\n    step_size = 0.95\n    eta_min = 1e-7\n    batch_size = 64\n    num_workers = 4\n#create_df(Config.false_path, Config.true_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Custom Dataset","metadata":{}},{"cell_type":"code","source":"# Augmentation \nIMAGE_SIZE = 256\ntrain_transforms = A.Compose([\n    A.RandomResizedCrop(IMAGE_SIZE, IMAGE_SIZE, scale=(0.9, 0.9), p=1),\n    A.OneOf([\n        A.MotionBlur(blur_limit=(3, 5)),\n        A.MedianBlur(blur_limit=5),\n        A.GaussianBlur(blur_limit=(3, 5)),\n        A.GaussNoise(var_limit=(5.0, 30.0)),\n        A.MultiplicativeNoise(),\n        A.Blur()\n    ], p=0.7),\n    A.GridDistortion(num_steps=5, distort_limit=1., p = 0.5),\n    A.CLAHE(clip_limit=4.0, p=0.7),\n    A.IAAPiecewiseAffine(p=0.2),\n    A.IAASharpen(p=0.2),\n    A.RandomGamma(gamma_limit=(70, 130), p=0.3),\n    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.75),\n    A.OneOf([\n        A.ImageCompression(),\n        A.Downscale(scale_min=0.7, scale_max=0.95),\n    ], p=0.2),\n    A.Cutout(),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85)\n])\ntest_transforms = A.Compose([\n    A.Normalize(),\n    ToTensorV2()\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ValDataset(torch.utils.data.Dataset):\n    def __init__(self, df):\n        self.df = df\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = row.image\n        covid = torch.tensor(row.covid)\n        # load in the image\n        image = lycon.load(image_path)\n        image = test_transforms(image = image)['image']\n        return image, covid\nclass TrainDataset(torch.utils.data.Dataset):\n    def __init__(self, df, num_samples):\n        self.df = df \n        self.num_samples = num_samples\n        self.actual_length = len(self.df)\n    def __len__(self):\n        return self.num_samples\n    def __getitem__(self, idx):\n        idx = random.randint(0, self.actual_length - 1)\n        row = self.df.iloc[idx]\n        image_path = row.image\n        covid = torch.tensor(row.covid)\n        \n        image = lycon.load(image_path)\n        image = train_transforms(image = image)['image']\n        image = test_transforms(image = image)['image']\n        return image, covid\nclass DataModule:\n    @classmethod\n    def get_both(cls):\n        train_dataset = TrainDataset(Config.train, Config.num_train_samples)\n        val_dataset = ValDataset(Config.test)\n        # Create DataLoaders\n        return train_dataset, val_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Structure\n- Modified EfficientNet + ViT Model, where I use EfficientNetB0 as a BackBone and ViT blocks for final processing ","metadata":{}},{"cell_type":"code","source":"class Mish(pl.LightningModule):\n    # Mish Activation Fn\n    def __init_(self):\n        super().__init__()\n    def forward(self, x):\n        return x * torch.tanh(F.softplus(x))\ndef replace_all_act(model):\n    for name, module in model.named_modules():\n        if isinstance(module, (nn.ReLU, nn.SiLU)):\n            setattr(model, name, Mish())\n        else:\n            replace_all_act(module)\ndef initialize_weights(layer):\n    # Better Initialization of CNN weights.\n    for m in layer.modules():\n        if isinstance(m, nn.Conv2d):\n            # Kaiming Init\n            act_type = ModelConfig.act_type\n            nn.init.kaiming_normal_(m.weight, nonlinearity = 'relu')\n\n        elif isinstance(m, nn.BatchNorm2d):\n            # 1's and 0's\n            m.weight.data.fill_(1)\n            m.bias.data.zero_()\n\n# Regular CNN Blocks for BaseLine\nclass Act(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.act_type = ModelConfig.act_type\n        if self.act_type == 'relu':\n            self.act = nn.ReLU(inplace = True)\n        else:\n            self.act = nn.SiLU(inplace = True) # Can be worse, but is faster\n    def forward(self, x):\n        return self.act(x)\nclass ConvBlock(pl.LightningModule):\n    def __init__(self, in_features, out_features, kernel_size, padding, groups, stride):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride, bias = False)\n        self.bn = nn.BatchNorm2d(out_features)\n        self.act = Act()\n        initialize_weights(self)\n    def forward(self, x):\n        return self.bn(self.act(self.conv(x)))\nclass EnhancedBN(pl.LightningModule):\n    # Enhanced Batch Normalization Block, using Conv2d instead of BN\n    def __init__(self, in_features):\n        super().__init__()\n        self.in_features = in_features\n        self.kernel_size = 3\n        self.padding = 1\n        \n        self.enhance_bn = ModelConfig.enhance_bn\n        if self.enhance_bn:\n            self.bn1 = nn.BatchNorm2d(self.in_features, affine = False)\n            self.bn2 = nn.Conv2d(self.in_features, self.in_features, kernel_size = self.kernel_size, padding = self.padding)\n        else:\n            self.bn1 = nn.BatchNorm2d(self.in_features)\n            self.bn2 = nn.Identity()\n        initialize_weights(self)\n    def forward(self, x):\n        return self.bn2(self.bn1(x))\nclass EnhancedConvBlock(pl.LightningModule):\n    def __init__(self, in_features, out_features, kernel_size, padding, groups, stride):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride, bias = False)\n        self.bn = EnhancedBN(out_features)\n        self.act = Act()\n        initialize_weights(self)\n    def forward(self, x):\n        return self.bn(self.act(self.conv(x)))\nclass SqueezeExcite(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        \n        self.Squeeze = nn.Linear(self.in_features, self.inner_features)\n        self.Act = Act()\n        self.Excite = nn.Linear(self.inner_features, self.in_features)\n    def forward(self, x):\n        mean = torch.mean(x, dim = -1)\n        mean = torch.mean(mean, dim = -1)\n        \n        squeeze = self.Act(self.Squeeze(mean))\n        excite = torch.sigmoid(self.Excite(squeeze)).unsqueeze(-1).unsqueeze(-1)\n        return excite * x\nclass SpatialAttention(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        \n        self.Squeeze = nn.Linear(self.in_features, self.inner_features)\n        self.Act = Act()\n        self.Excite = nn.Linear(self.inner_features, self.in_features)\n        \n        self.Conv = nn.Conv2d(self.in_features, self.inner_features, 1)\n        initialize_weights(self)\n    def forward(self, x):\n        mean = torch.mean(x, dim = -1)\n        mean = torch.mean(mean, dim = -1) \n        \n        squeeze = self.Act(self.Squeeze(mean))\n        excite = torch.sigmoid(self.Excite(squeeze)).unsqueeze(-1).unsqueeze(-1) * x\n        \n        squeeze_conv = torch.sigmoid(self.Conv(x)) * x\n        return (excite + squeeze_conv) / 2\nclass ECASqueezeExcite(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.kernel_size = 5\n        self.padding = 2\n        self.avgPool = nn.AdaptiveAvgPool2d((1, 1))\n        self.conv2d = nn.Conv2d(1, 1, kernel_size = self.kernel_size, padding = self.padding, bias = False)\n    def forward(self, x):\n        pooled = torch.squeeze(self.avgPool(x), dim = -1).transpose(-1, -2) # (B, 1, C)\n        conv = torch.sigmoid(self.conv2d(pooled)).transpose(-1, -2).unsqueeze(-1)\n        return conv * x\nclass TransformerSqueezeExcite(pl.LightningModule):\n    '''\n    Like in T2TVit, SqueezeExcite module for Transformers\n    '''\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        \n        self.Squeeze = nn.Linear(self.in_features, self.inner_features)\n        self.Act = Act()\n        self.Excite = nn.Linear(self.inner_features, self.in_features)\n        \n        self.gate_attention = ModelConfig.gate_attention\n        if self.gate_attention:\n            self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        '''\n        x: Tensor(B, L, C)\n        '''\n        squeeze = self.Act(self.Squeeze(x))\n        excited = torch.sigmoid(self.Excite(squeeze)) * x\n        if self.gate_attention:\n            gamma = torch.sigmoid(self.gamma)\n            return gamma * excited + (1 - gamma) * x\n        return excited\n        \n    \nclass SelfAttention(pl.LightningModule):\n    '''\n    Self Attention for ViT\n    \n    Full O(N^2) Attention: Performer Attention outsourced to pip.\n    '''\n    def __init__(self, in_features, inner_features, num_heads):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.num_heads = num_heads\n        \n        self.Keys = nn.Linear(self.in_features, self.inner_features * self.num_heads)\n        self.Queries = nn.Linear(self.in_features, self.inner_features * self.num_heads)\n        self.Values = nn.Linear(self.in_features, self.inner_features * self.num_heads)\n        self.Linear = nn.Linear(self.inner_features * self.num_heads, self.in_features)\n    def forward(self, x):\n        K = self.Keys(x)\n        V = self.Queries(x)\n        Q = self.Values(x) # (B, L, HI)\n        \n        K = K.reshape(B, L, self.num_heads, self.inner_features)\n        V = V.reshape(B, L, self.num_heads, self.inner_features)\n        Q = Q.reshape(B, L, self.num_heads, self.inner_features)\n        \n        K = K.transpose(1, 2).reshape(B * self.num_heads, L, self.inner_features)\n        V = V.transpose(1, 2).reshape(B * self.num_heads, L, self.inner_features)\n        Q = Q.transpose(1, 2).reshape(B * self.num_heads, L, self.inner_features)\n        \n        att_mat = F.softmax(torch.bmm(K, V.transpose(1, 2)) / math.sqrt(self.inner_features))\n        att_scores = torch.bmm(att_mat, Q)\n        \n        att_scores = att_scores.reshape(B, self.num_heads, L, self.inner_features)\n        att_scores = att_scores.transpose(1, 2).transpose(B, L, self.num_heads * self.inner_features)\n        return self.Linear(att_scores)\nclass PerformerSelfAttention(pl.LightningModule):\n    # Performer O(N) attention, uses fancy matrix operations to perform attention quickly.\n    def __init__(self, in_features, inner_features, num_heads):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.num_heads = num_heads\n        \n        self.K = nn.Linear(self.in_features, self.inner_features * self.num_heads)\n        self.V = nn.Linear(self.in_features, self.inner_features * self.num_heads)\n        self.Q = nn.Linear(self.in_features, self.inner_features * self.num_heads)\n        \n        self.Linear = nn.Linear(self.inner_features * self.num_heads, self.in_features)\n        self.FastAttention = performer_pytorch.FastAttention(\n            dim_heads = self.inner_features, \n            nb_features = self.inner_features * 2\n        )\n    def forward(self, x):\n        B, L, C = x.shape\n        K = self.K(x)\n        V = self.V(x)\n        Q = self.Q(x)\n        \n        K = K.reshape(B, L, self.num_heads, self.inner_features)\n        V = V.reshape(B, L, self.num_heads, self.inner_features)\n        Q = Q.reshape(B, L, self.num_heads, self.inner_features)\n        \n        K = K.transpose(1, 2)\n        V = V.transpose(1, 2)\n        Q = Q.transpose(1, 2)\n        \n        attended = self.FastAttention(Q, K, V)\n        \n        attended = attended.transpose(1, 2)\n        attended = attended.reshape(B, L, self.num_heads, self.inner_features)\n        attended = attended.reshape(B, L, self.num_heads * self.inner_features)\n        \n        return self.Linear(attended)\nclass TransformerAttention(pl.LightningModule):\n    def __init__(self, in_features, inner_features, num_heads):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.num_heads = num_heads\n        \n        self.attention_type = ModelConfig.transformer_attention\n        if self.attention_type == 'performer':\n            self.layer = PerformerSelfAttention(self.in_features, self.inner_features, self.num_heads)\n        else:\n            self.layer = SelfAttention(self.in_features, self.inner_features, self.num_heads)\n    def forward(self, x):\n        return self.layer(x)\nclass TransformerEncoder(pl.LightningModule):\n    def __init__(self, in_features):\n        super().__init__()\n        self.in_features = in_features\n        self.reduction = ModelConfig.reduction \n        self.inner_features = self.in_features // self.reduction \n        self.max_length = ModelConfig.max_length \n        \n        self.LayerNorm1 = nn.LayerNorm((self.max_length, self.in_features))\n        self.Attention = TransformerAttention(self.in_features, self.inner_features, self.reduction)\n        self.LayerNorm2 = nn.LayerNorm((self.max_length, self.in_features))\n        self.Linear = nn.Linear(self.in_features, self.in_features)\n    def forward(self, x):\n        norm1 = self.LayerNorm1(x)\n        attention = self.Attention(norm1) + x\n        \n        norm2 = self.LayerNorm2(attention)\n        linear = self.Linear(norm2) + attention\n        return linear\nclass Attention(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.attention_type = ModelConfig.attention_type\n        assert self.attention_type in ['scse', 'none', 'se', 'eca']\n        if self.attention_type == 'eca':\n            self.layer = ECASqueezeExcite()\n        elif self.attention_type == 'scse':\n            self.layer = SpatialAttention(in_features, inner_features)\n        elif self.attention_type == 'none':\n            self.layer = nn.Identity()\n        else:\n            self.layer = SqueezeExcite(in_features, inner_features)\n        self.gate_attention = ModelConfig.gate_attention\n        if self.gate_attention:\n            self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        layer = self.layer(x)\n        if self.gate_attention:\n            gamma = torch.sigmoid(self.gamma)\n            return gamma * layer + (1 - gamma) * x\n        return layer\nclass AstrousConvolutionalBlock(pl.LightningModule):\n    def __init__(self, in_features, out_features, kernel_size, padding, groups, stride, dilation):\n        super().__init__()\n        self.conv = nn.Conv2d(in_features, out_features, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride, dilation = dilation, bias = False)\n        self.bn = nn.BatchNorm2d(out_features)\n        self.act = Act()\n        initialize_weights(self)\n    def forward(self, x):\n        return self.bn(self.act(self.conv(x)))\n    \nclass BAM(pl.LightningModule):\n    # BAM modules to be placed in between bottleneck layers in Encoder.\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        \n        self.squeeze = nn.Linear(self.in_features, self.inner_features)\n        self.act = Act()\n        self.excite = nn.Linear(self.inner_features, self.in_features)\n        self.dilation = ModelConfig.dilation\n        \n        self.squeeze_conv = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.dw = AstrousConvolutionalBlock(self.inner_features, self.inner_features, 3, self.dilation, self.inner_features, 1, self.dilation)\n        self.excite_conv = ConvBlock(self.inner_features, 1, 1, 0, 1, 1)\n        \n        self.gate_attention = ModelConfig.gate_attention\n        if self.gate_attention:\n            self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        mean = torch.mean(x, dim = -1)\n        mean = torch.mean(mean, dim = -1)\n        \n        squeeze = self.act(self.squeeze(mean))\n        excite = self.excite(squeeze).unsqueeze(-1).unsqueeze(-1)\n        \n        squeeze_conv = self.squeeze_conv(x)\n        dw = self.dw(squeeze_conv)\n        excite_conv = self.excite_conv(dw)\n        \n        excite = torch.sigmoid((excite + excite_conv) / 2) * x\n        if self.gate_attention:\n            gamma = torch.sigmoid(self.gamma)\n            return gamma * excite + (1 - gamma) * x # gated resnet block.\n        return excite + x # Just Simple Add.\n        \nclass BottleNeck(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.reduction = ModelConfig.reduction\n        \n        self.Squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.Process = ConvBlock(self.inner_features, self.inner_features, 3, 1, 1, 1)\n        self.Expand = EnhancedConvBlock(self.inner_features, self.in_features, 1, 0, 1, 1)\n        self.SE = Attention(self.in_features, self.inner_features // self.reduction)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        squeeze = self.Squeeze(x)\n        process = self.Process(squeeze)\n        expand = self.Expand(process)\n        SE = self.SE(expand)\n    \n        gamma = torch.sigmoid(self.gamma)\n        return gamma * SE + (1 - gamma) * x\nclass BottleNeckInverse(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.reduction = ModelConfig.reduction\n        \n        self.Expand = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.DW = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features, 1)\n        self.SE = Attention(self.inner_features, self.inner_features // self.reduction)\n        self.Squeeze = EnhancedConvBlock(self.inner_features, self.in_features, 1, 0, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        expand = self.Expand(x)\n        dw = self.DW(expand)\n        se = self.SE(dw)\n        squeeze = self.Squeeze(se)\n        \n        gamma = torch.sigmoid(self.gamma)\n        return gamma * squeeze + (1 - gamma) * x\nclass DownSamplerBottleNeck(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, stride):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.stride = stride\n        self.reduction = ModelConfig.reduction\n        \n        self.avg_pool = nn.AvgPool2d(kernel_size = 3, padding = 1, stride = self.stride)\n        self.conv_pool = EnhancedConvBlock(self.in_features, self.out_features, 3, 1, 1, 1)\n        \n        self.Squeeze = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.Process = ConvBlock(self.inner_features, self.inner_features, 3, 1, 1, 1)\n        self.Expand = EnhancedConvBlock(self.inner_features, self.out_features, 1, 0, 1, 1)\n        self.SE = Attention(self.out_features, self.out_features // self.reduction)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        pooled = self.avg_pool(x)\n        conv_pool = self.conv_pool(pooled)\n        \n        squeeze = self.Squeeze(pooled)\n        process = self.Process(squeeze)\n        expand = self.Expand(process)\n        SE = self.SE(expand)\n        \n        gamma = torch.sigmoid(self.gamma)\n        return gamma * SE + (1  - gamma) * conv_pooled\nclass DownSamplerBottleNeckInverse(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, stride):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        \n        self.stride = stride\n        self.reduction = ModelConfig.reduction\n        \n        self.avg_pool = nn.AvgPool2d(kernel_size = 3, padding = 1, stride = self.stride)\n        self.conv_pool = EnhancedConvBlock(self.in_features, self.out_features, 3, 1, 1, 1)\n        \n        self.Expand = ConvBlock(self.in_features, self.inner_features, 1, 0, 1, 1)\n        self.Process = ConvBlock(self.inner_features, self.inner_features, 3, 1, self.inner_features, 1)\n        self.SE = Attention(self.inner_features, self.inner_features // self.reduction)\n        self.Squeeze = EnhancedConvBlock(self.inner_features, self.out_features, 1, 0, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        pooled = self.avg_pool(x)\n        conv_pool = self.conv_pool(pooled)\n        \n        expand = self.Expand(pooled)\n        process = self.Process(expand)\n        se = self.SE(process)\n        squeeze = self.Squeeze(se)\n        \n        gamma = torch.sigmoid(self.gamma)\n        return gamma * squeeze + (1 - gamma) * conv_pool\nclass FusedMBConv(pl.LightningModule):\n    # Fused MB Conv Blocks\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features \n        self.reduction = ModelConfig.reduction\n        \n        self.ConvFused = ConvBlock(self.in_features, self.inner_features, 3, 1, 1, 1)\n        self.SE = Attention(self.inner_features, self.inner_features // self.reduction)\n        self.ConvProj = EnhancedConvBlock(self.inner_features, self.in_features, 1, 0, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        fused = self.ConvFused(x)\n        SE = self.SE(fused)\n        proj = self.ConvProj(SE)\n        \n        gamma = torch.sigmoid(self.gamma)\n        return gamma * proj + (1 - gamma) * x\n        \nclass DownSamplerFusedMBConv(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, stride):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        \n        self.stride = stride\n        self.reduction = ModelConfig.reduction \n        self.pool = nn.AvgPool2d(kernel_size = 3, padding = 1, stride = self.stride)\n        self.conv_pool = EnhancedConvBlock(self.in_features, self.out_features, 3, 1, 1, 1)\n        \n        self.ConvFused = ConvBlock(self.in_features, self.inner_features, 3, 1, 1, 1)\n        self.SE = Attention(self.inner_features, self.inner_features // self.reduction)\n        self.ConvProj = EnhancedConvBlock(self.inner_features, self.out_features, 1, 0, 1, 1)\n        \n        self.gamma = nn.Parameter(torch.zeros((1), device = self.device))\n    def forward(self, x):\n        pooled = self.pool(x)\n        conv_pool = self.conv_pool(pooled)\n        \n        ConvFused = self.ConvFused(pooled)\n        SE = self.SE(ConvFused)\n        ConvProj = self.ConvProj(SE)\n        \n        gamma = torch.sigmoid(self.gamma)\n        return gamma * ConvProj + (1 - gamma) * conv_pool\nclass ChooseBottleNeck(pl.LightningModule):\n    def __init__(self, in_features, inner_features):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.BottleNeck_Type = ModelConfig.bottleneck_type\n        assert self.BottleNeck_Type in ['inverse', 'fused', 'bottleneck', 'none']\n        if self.BottleNeck_Type == 'inverse':\n            self.layer = BottleNeckInverse(self.in_features, self.inner_features)\n        elif self.BottleNeck_Type == 'fused':\n            self.layer = FusedMBConv(self.in_features, self.inner_features)\n        elif self.BottleNeck_Type == 'bottleneck':\n            self.layer = BottleNeck(self.in_featurs, self.inner_features)\n    def forward(self, x):\n        return self.layer(x)\nclass ChooseDownSampler(pl.LightningModule):\n    def __init__(self, in_features, inner_features, out_features, stride):\n        super().__init__()\n        self.in_features = in_features\n        self.inner_features = inner_features\n        self.out_features = out_features\n        self.stride = stride\n        self.BottleNeck_Type = ModelConfig.bottleneck_type\n        assert self.BottleNeck_Type in ['inverse', 'bottleneck', 'fused']\n        \n        if self.BottleNeck_Type == 'inverse':\n            self.layer = DownSamplerBottleNeckInverse(self.in_features, self.inner_features, self.out_features, self.stride)\n        elif self.BottleNeck_Type == 'fused':\n            self.layer = DownSamplerFusedMBConv(self.in_features, self.inner_features, self.out_features, self.stride)\n        else:\n            self.layer = DownSamplerBottleNeckInverse(self.in_features, self.inner_features, self.out_features, self.stride)\n    def forward(self, x):\n        return self.layer(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"# Model Construction","metadata":{}},{"cell_type":"code","source":"class ViTAlphaBackBone(pl.LightningModule):\n    def freeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = False\n    def unfreeze(self, layers):\n        for layer in layers:\n            for parameter in layer.parameters():\n                parameter.requires_grad = True\n    def __init__(self):\n        super().__init__()\n        self.model_name = 'efficientnet-b0'\n        self.model = EfficientNet.from_pretrained(self.model_name)\n        self.encoder_dims = [32, 16, 24, 40, 80, 112, 192, 320, ModelConfig.transformer_dim]\n        \n        \n        self.conv1 = self.model._conv_stem # 32\n        self.bn1 = self.model._bn0\n        self.act1 = self.model._swish\n        \n        self.block0 = nn.Sequential(*[self.model._blocks[0]])# 16\n        self.block1 = nn.Sequential(*self.model._blocks[1:3]) # 24, downsampled\n        self.block2 = nn.Sequential(*self.model._blocks[3:5]) # 40, downsampled\n        self.block3 = nn.Sequential(*self.model._blocks[5:8]) # 80, downsampled\n        self.block4 = nn.Sequential(*self.model._blocks[8:11]) # 112\n        self.block5 = nn.Sequential(*self.model._blocks[11:15]) # 192, downsampled\n        self.block6 = nn.Sequential(*self.model._blocks[15:]) # 320\n        \n        # Freeze Layers\n        self.freeze([self.conv1, self.bn1, self.block0, self.block1, self.block2, self.block3, self.block4])\n        # Additional Layers\n        self.reduction = ModelConfig.reduction\n        \n        self.Attention6 = BAM(self.encoder_dims[7], self.encoder_dims[7] // self.reduction)\n        self.Dropout6 = nn.Dropout2d(0.1)\n        \n        self.expand = ModelConfig.expand\n        self.num_blocks = ModelConfig.num_blocks\n        self.block7 = nn.Sequential(*[\n            ChooseDownSampler(self.encoder_dims[7], self.encoder_dims[7] * self.expand, self.encoder_dims[8], 2)\n        ] + [\n            ChooseBottleNeck(self.encoder_dims[8], self.encoder_dims[8] * self.expand) for i in range(self.num_blocks)\n        ])\n        self.Attention7 = BAM(self.encoder_dims[8], self.encoder_dims[8] // self.reduction)\n        self.Dropout7 = nn.Dropout2d(0.1)\n    def forward(self, x):\n        features0 = self.bn1(self.act1(self.conv1(x))) # 128\n        block0 = self.block0(features0) # 128\n        block1 = self.block1(block0) # 64\n        block2 = self.block2(block1) # 32\n        block3 = self.block3(block2) # 16\n        block4 = self.block4(block3) # 16\n        block5 = self.block5(block4) # 8\n        block6 = self.block6(block5) # 8\n        \n        block6 = self.Dropout6(block6)\n        block6 = self.Attention6(block6)\n        \n        block7 = self.block7(block6)\n        block7 = self.Dropout7(block7)\n        block7 = self.Attention7(block7) # 4\n        return block7","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BaselineHead(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.input_dim = ModelConfig.transformer_dim\n        self.num_classes = ModelConfig.num_classes\n        self.global_avg = nn.AdaptiveAvgPool2d((1, 1))\n        self.Linear = nn.Linear(self.input_dim, self.num_classes)\n    def forward(self, x):\n        mean = torch.squeeze(self.global_avg(x))\n        return self.Linear(mean)\nclass ViTAlphaTransformer(pl.LightningModule):\n    '''\n    Adds a Few Vision Transformer layers on top of the CNN\n    '''\n    def __init__(self):\n        super().__init__()\n        self.input_dim = ModelConfig.transformer_dim\n        self.num_classes = ModelConfig.num_classes\n        self.num_encoders = ModelConfig.num_encoders\n\n        self.encoders = nn.Sequential(*[\n            TransformerEncoder(self.input_dim) for i in range(self.num_encoders)\n        ])\n        self.max_length = ModelConfig.max_length\n        self.pos_enc = self.positional_encodings().unsqueeze(0)\n        self.Linear = nn.Linear(self.input_dim, self.num_classes)\n    def positional_encodings(self):\n        # precomputes the positional_encodings\n        L, C = (self.max_length, self.input_dim)\n        pos_enc = torch.zeros((L, C), device = self.device)\n        for pos in range(L):\n            for i in range(0, pos, 2):\n                pos_enc[pos, i] = math.sin(pos / 10000 ** (2 * i / self.input_dim))\n                pos_enc[pos, i + 1] = math.cos(pos / 10000 ** (2 * (i + 1) / self.input_dim))\n        return pos_enc\n    def forward(self, x):\n        # X: Tensor(B, 512, 4, 4)\n        B, C, H, W = x.shape\n        x = x.view(B, C, -1).transpose(-1, -2) # (B, 16, 512)\n        # add Positional encodings\n        x = x + torch.repeat_interleave(self.pos_enc, B, dim = 0).to(self.device)\n        # Encode Using transformers\n        features = self.encoders(x) # (B, 16, 512)\n        # average over tokens(This is due to how attention is local now using Performer attention, so we want global features)\n        mean = torch.mean(features, dim = 1) # (B, 512)\n        return self.Linear(mean) # (B, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ViTAlpha(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.BackBone = ViTAlphaBackBone()\n        self.head = BaselineHead() if ModelConfig.model_type == 'baseline' else ViTAlphaTransformer()\n        self.use_mish = ModelConfig.act_type == 'mish'\n        if self.use_mish:\n            replace_all_act(self.BackBone)\n    def forward(self, x):\n        features = self.BackBone(x)\n        return self.head(features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ModelConfig:\n    # Configurations of the Model\n    model_type = 'transformer'\n    act_type = 'relu'\n    enhance_bn = False # To Test\n    num_classes = 2 # Often times, CE > BCE.\n    \n    gate_attention = True # Whether or not to gate attention, makes attention less unstable. \n    attention_type = 'se'\n    bottleneck_type = 'inverse'\n    transformer_attention = 'performer' # Performer is more memory efficient, faster, etc.\n    num_encoders = 2\n    reduction = 4\n    dilation = 4\n    expand = 2\n    num_blocks = 3\n    max_length = 16 # Not really a hyper parameter, decided based on flattened size after CNN encodings\n    transformer_dim = 512","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Code and Metrics","metadata":{}},{"cell_type":"code","source":"class Store():\n    # Stores States for Metric\n    def __init__(self, pred, y):\n        self.pred = pred\n        self.y = y\nclass Accuracy(Metric):\n    def __init__(self):\n        self.Accuracy = 0\n        self.count = 0\n    def reset(self):\n        self.Accuracy = 0\n        self.count = 0\n    def accumulate(self, learn):\n        y_pred, y_true = learn.pred, learn.y\n        # Y_Pred: (B, 2)\n        y_pred = F.softmax(y_pred, dim = -1) # (B, 2)\n        _, y_pred = torch.max(y_pred, dim = -1) # (B)\n        B = y_pred.shape[0]\n        \n        accuracy = torch.sum(y_pred == y_true) / B\n        self.Accuracy += accuracy.item()\n        self.count += 1\n    @property\n    def value(self):\n        if self.count != 0:\n            return round(self.Accuracy / self.count)\n        return 0\nclass F1Score(Metric):\n    def __init__(self):\n        self.f_score = 0\n        self.count = 0\n    def reset(self):\n        self.f_score = 0\n        self.count = 0\n    def accumulate(self, learn):\n        y_pred, y_true = learn.pred, learn.y\n        y_pred = F.softmax(y_pred, dim = -1)\n        _, y_pred = torch.max(y_pred, dim = -1)\n        f_score = metrics.f1_score(y_true.detach().cpu(), y_pred.detach().cpu(), average = 'micro')\n        self.f_score += f_score\n        self.count += 1\n    @property\n    def value(self):\n        if self.count != 0:\n            return round(self.f_score / self.count, 3)\n        return 0 \nclass Loss(Metric):\n    def __init__(self):\n        self.criterion = nn.CrossEntropyLoss()\n        self.loss = 0\n        self.count = 0\n    def reset(self):\n        self.loss = 0\n        self.count = 0\n    def accumulate(self, learn):\n        y_pred, y_true = learn.pred, learn.y\n        loss = self.criterion(y_pred, y_true)\n        self.loss += loss.item()\n        self.count += 1\n        return loss\n    @property\n    def value(self):\n        if self.count != 0:\n            return round(self.loss / self.count, 3)\n        return 999","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TrainingConfig","metadata":{}},{"cell_type":"code","source":"class TrainingConfig:\n    lr = 1e-3\n    weight_decay = 1e-1\n    eta_min = 1e-8\n    num_steps = 5\n    factor = 0.1\n    patience = 3\n    \n    batch_size = 64\n    num_epochs = 80\n    num_workers = 4\n    grad_clip = 20","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainerModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = self.configure_model()\n        # Initialize States\n        self.best = {'val_loss': float('inf'), 'val_f1': 0, 'val_accuracy': 0}\n        self.EPOCHS = -1\n        self.TrainLoss = Loss()\n        self.ValLoss = Loss()\n        self.ValAccuracy = Accuracy()\n        self.ValF1 = F1Score()\n        self.reset()\n    def reset(self):\n        self.EPOCHS += 1\n        self.TrainLoss.reset()\n        self.ValLoss.reset()\n        self.ValAccuracy.reset()\n        self.ValF1.reset()\n    def configure_model(self):\n        model = ViTAlpha()\n        return model\n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.model.parameters(), lr = TrainingConfig.lr, weight_decay = TrainingConfig.weight_decay)\n        self.lr_decay_plateau = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'max', factor = TrainingConfig.factor, min_lr = TrainingConfig.eta_min, patience = TrainingConfig.patience, verbose = True)\n        self.lr_decay_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, TrainingConfig.num_steps, eta_min = TrainingConfig.eta_min)\n        return optimizer\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        # already on GPU\n        pred = self.model(x)\n        store = Store(pred, y)\n        loss = self.TrainLoss.accumulate(store)\n        return loss\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        pred = self.model(x)\n        store = Store(pred, y)\n        loss = self.ValLoss.accumulate(store)\n        self.ValAccuracy.accumulate(store)\n        self.ValF1.accumulate(store)\n        return loss\n    def process_states(self):\n        trainLoss = self.TrainLoss.value\n        valLoss = self.ValLoss.value\n        valAccuracy = self.ValAccuracy.value\n        valF1 = self.ValF1.value\n        self.log('val_f', valF1)\n        # take steps\n        self.lr_decay_cosine.step()\n        self.lr_decay_plateau.step(valF1)\n        \n        if valLoss <= self.best['val_loss']:\n            self.best['val_loss'] = valLoss\n            torch.save(self.state_dict(), './loss.pth')\n        if valF1 >= self.best['val_f1']:\n            self.best['val_f1'] = valF1\n            torch.save(self.state_dict(), './f1.pth')\n        if valAccuracy >= self.best['val_accuracy']:\n            self.best['val_accuracy'] = valAccuracy\n            torch.save(self.state_dict(), \"./acc.pth\")\n        print(f\"E: {self.EPOCHS}, BL: {self.best['val_loss']} BF: {self.best['val_f1']} BA: {self.best['val_accuracy']} TL: {trainLoss} VL: {valLoss} VA: {valAccuracy} VF: {valF1}\")\n    def validation_epoch_end(self, logs):\n        self.process_states()\n        self.reset()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_full():\n    model = TrainerModel()\n    train, val = DataModule.get_both()\n    dls = DataLoaders.from_dsets(train, val, pin_memory = True, num_workers= TrainingConfig.num_workers, batch_size = TrainingConfig.batch_size, worker_init_fn = seed_worker)\n    if torch.cuda.is_available: model.cuda(), dls.cuda()\n    cbs = [pl.callbacks.EarlyStopping(monitor = 'val_f', mode = 'max', patience = 5)]\n    trainer = pl.Trainer(gpus = 1, num_sanity_val_steps = 0, check_val_every_n_epoch = 1, logger = None, checkpoint_callback = False, callbacks = cbs, benchmark = False, deterministic = True, gradient_clip_val = TrainingConfig.grad_clip, max_epochs = TrainingConfig.num_epochs, precision = 16)\n    trainer.fit(model, dls[0], dls[1])\n    del model\n    del train, val\n    del dls\n    del trainer\n    torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_full()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Code","metadata":{}},{"cell_type":"code","source":"class TestingModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = self.configure_model()\n    def configure_model(self):\n        model = ViTAlpha()\n        return model\n    def forward(self, x):\n        self.eval()\n        with torch.no_grad():\n            pred = F.softmax(self.model(x))\n            _, pred = torch.max(pred, dim = -1)\n            return pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, img_file):\n    image = cv2.imread(img_file)\n    plt.imshow(img_file)\n    plt.show()\n    image = test_transforms(image = image)['image']\n    pred = torch.squeeze(model(image.unsqueeze(0))).item()\n    if pred:\n        print(\"COVID\")\n    else:\n        print(\"NO COVID\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, val = DataModule.get_both()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TestingModel()\nmodel.load_state_dict(torch.load('../input/ctscantrained/f1.pth', map_location = device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = 0\nfor image, label in val:\n    print('----------------------------------')\n    plt.imshow(image.transpose(0, 1).transpose(1, 2))\n    plt.show()\n    print('--------GT---------------')\n    print(label.item() == 1)\n    pred = model(image.unsqueeze(0))\n    print('------------Pred-----------------')\n    print(torch.squeeze(pred).item() == 1)\n    count += 1\n    print('--------------------------------------')\n    if count == 32:\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}